"""
é…·æ­Œè©çˆ¬èŸ² - åœ–å½¢ç•Œé¢æ‡‰ç”¨
åŠŸèƒ½ï¼š
1. æŒ‰é æ•¸ç¯„åœçˆ¬å–
2.é¸æ“‡çˆ¬å–é¡å‹:æ­Œæ‰‹ã€ä½œè©ã€ä½œæ›²
3. æŒ‰é¡å‹äººåçˆ¬å–
4.è¼¸å…¥äººåæˆ–é æ•¸
5. é¸æ“‡ä¿å­˜ä½ç½®
"""

import tkinter as tk
from tkinter import filedialog, messagebox, scrolledtext
import requests
from bs4 import BeautifulSoup
import os
import re
import time
import threading
from urllib.parse import urljoin

class LyricsCrawlerApp:
    def __init__(self, root):
        """åˆå§‹åŒ–çˆ¬èŸ²æ‡‰ç”¨ç¨‹å¼"""
        self.root = root
        self.root.title("é…·æ­Œè©çˆ¬èŸ² v1.0")
        self.root.geometry("700x600")
        self.root.resizable(False, False)
        
        # çˆ¬èŸ²é…ç½®
        self.base_url = "https://www.kugeci.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        # è«‹æ±‚ç©©å®šæ€§è¨­ç½®
        self.request_timeout = 15
        self.max_retries = 3
        self.backoff_base = 1.8  # æŒ‡æ•¸é€€é¿åº•æ•¸
        self.jitter_seconds = 0.3
        # å…±ç”¨ Session ä»¥æå‡é€£ç·šé‡ç”¨èˆ‡ç©©å®šæ€§
        import requests as _req
        self.session = _req.Session()
        self.session.headers.update(self.headers)
        self.is_crawling = False
        
        self.create_widgets()
    
    def create_widgets(self):
        """å‰µå»ºç•Œé¢çµ„ä»¶"""
        
        # æ¨™é¡Œ
        title_frame = tk.Frame(self.root, bg="#4A90E2", height=60)
        title_frame.pack(fill=tk.X)
        title_frame.pack_propagate(False)
        
        title_label = tk.Label(
            title_frame, 
            text="ğŸµ é…·æ­Œè©çˆ¬èŸ²å·¥å…·", 
            font=("å¾®è»Ÿé›…é»‘", 18, "bold"),
            bg="#4A90E2", 
            fg="white"
        )
        title_label.pack(pady=15)
        
        # ä¸»å®¹å™¨
        main_frame = tk.Frame(self.root, padx=20, pady=20)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # é¡å‹é¸æ“‡
        type_frame = tk.LabelFrame(main_frame, text="é¸æ“‡çˆ¬å–é¡å‹", font=("å¾®è»Ÿé›…é»‘", 11, "bold"), padx=10, pady=10)
        type_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.type_var = tk.StringVar(value="writer")
        
        tk.Radiobutton(
            type_frame, 
            text="ğŸ‘¤ ä½œè©äºº", 
            variable=self.type_var, 
            value="writer",
            font=("å¾®è»Ÿé›…é»‘", 10),
            command=self.toggle_type
        ).grid(row=0, column=0, sticky=tk.W, pady=5, padx=(0, 20))
        
        tk.Radiobutton(
            type_frame, 
            text="ğŸ¼ ä½œæ›²äºº", 
            variable=self.type_var, 
            value="composer",
            font=("å¾®è»Ÿé›…é»‘", 10),
            command=self.toggle_type
        ).grid(row=0, column=1, sticky=tk.W, pady=5, padx=(0, 20))
        
        tk.Radiobutton(
            type_frame, 
            text="ğŸ¤ æ­Œæ‰‹", 
            variable=self.type_var, 
            value="singer",
            font=("å¾®è»Ÿé›…é»‘", 10),
            command=self.toggle_type
        ).grid(row=0, column=2, sticky=tk.W, pady=5)
        
        # çˆ¬å–æ¨¡å¼é¸æ“‡
        mode_frame = tk.LabelFrame(main_frame, text="çˆ¬å–æ¨¡å¼", font=("å¾®è»Ÿé›…é»‘", 11, "bold"), padx=10, pady=10)
        mode_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.mode_var = tk.StringVar(value="name")
        
        tk.Radiobutton(
            mode_frame, 
            text="ğŸ“ æŒ‰äººåçˆ¬å–", 
            variable=self.mode_var, 
            value="name",
            font=("å¾®è»Ÿé›…é»‘", 10),
            command=self.toggle_mode
        ).grid(row=0, column=0, sticky=tk.W, pady=5, padx=(0, 20))
        
        tk.Radiobutton(
            mode_frame, 
            text="ğŸ“„ æŒ‰é æ•¸ç¯„åœçˆ¬å–", 
            variable=self.mode_var, 
            value="pages",
            font=("å¾®è»Ÿé›…é»‘", 10),
            command=self.toggle_mode
        ).grid(row=0, column=1, sticky=tk.W, pady=5)
        
        # åƒæ•¸è¼¸å…¥å€
        param_frame = tk.LabelFrame(main_frame, text="åƒæ•¸è¨­ç½®", font=("å¾®è»Ÿé›…é»‘", 11, "bold"), padx=10, pady=10)
        param_frame.pack(fill=tk.X, pady=(0, 10))
        
        # äººåè¼¸å…¥
        self.name_frame = tk.Frame(param_frame)
        self.name_frame.pack(fill=tk.X, pady=5)
        
        self.name_label = tk.Label(self.name_frame, text="äººå:", font=("å¾®è»Ÿé›…é»‘", 10))
        self.name_label.grid(row=0, column=0, padx=(0, 5))
        self.name_entry = tk.Entry(self.name_frame, width=30, font=("å¾®è»Ÿé›…é»‘", 10))
        self.name_entry.grid(row=0, column=1)
        
        self.name_hint = tk.Label(
            self.name_frame, 
            text="(ä¾‹: æ—å¤•ã€å‘¨æ°å€«ã€å‘¨æ·±)", 
            font=("å¾®è»Ÿé›…é»‘", 8),
            fg="gray"
        )
        self.name_hint.grid(row=0, column=2, padx=(5, 0))
        
        # é æ•¸ç¯„åœè¼¸å…¥
        self.page_frame = tk.Frame(param_frame)
        
        tk.Label(self.page_frame, text="èµ·å§‹é :", font=("å¾®è»Ÿé›…é»‘", 10)).grid(row=0, column=0, padx=(0, 5))
        self.start_page_entry = tk.Entry(self.page_frame, width=10, font=("å¾®è»Ÿé›…é»‘", 10))
        self.start_page_entry.grid(row=0, column=1, padx=(0, 20))
        self.start_page_entry.insert(0, "1")
        
        tk.Label(self.page_frame, text="çµæŸé :", font=("å¾®è»Ÿé›…é»‘", 10)).grid(row=0, column=2, padx=(0, 5))
        self.end_page_entry = tk.Entry(self.page_frame, width=10, font=("å¾®è»Ÿé›…é»‘", 10))
        self.end_page_entry.grid(row=0, column=3)
        self.end_page_entry.insert(0, "1")
        
        # å»¶é²è¨­ç½®
        delay_frame = tk.Frame(param_frame)
        delay_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(delay_frame, text="è«‹æ±‚å»¶é²:", font=("å¾®è»Ÿé›…é»‘", 10)).grid(row=0, column=0, padx=(0, 5))
        self.delay_entry = tk.Entry(delay_frame, width=10, font=("å¾®è»Ÿé›…é»‘", 10))
        self.delay_entry.grid(row=0, column=1)
        self.delay_entry.insert(0, "2.0")
        tk.Label(delay_frame, text="ç§’ (å»ºè­°2-3ç§’)", font=("å¾®è»Ÿé›…é»‘", 8), fg="gray").grid(row=0, column=2, padx=(5, 0))
        
        # ä¿å­˜ä½ç½®
        path_frame = tk.LabelFrame(main_frame, text="ä¿å­˜ä½ç½®", font=("å¾®è»Ÿé›…é»‘", 11, "bold"), padx=10, pady=10)
        path_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.path_var = tk.StringVar(value=os.path.join(os.getcwd(), "lyrics_output"))
        
        tk.Entry(
            path_frame, 
            textvariable=self.path_var, 
            font=("å¾®è»Ÿé›…é»‘", 9),
            state="readonly",
            width=60
        ).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 10))
        
        tk.Button(
            path_frame, 
            text="ğŸ“ é¸æ“‡", 
            command=self.select_folder,
            font=("å¾®è»Ÿé›…é»‘", 9),
            bg="#5CB85C",
            fg="white",
            cursor="hand2"
        ).pack(side=tk.RIGHT)
        
        # æ§åˆ¶æŒ‰éˆ•
        button_frame = tk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.start_button = tk.Button(
            button_frame,
            text="ğŸš€ é–‹å§‹çˆ¬å–",
            command=self.start_crawling,
            font=("å¾®è»Ÿé›…é»‘", 12, "bold"),
            bg="#4A90E2",
            fg="white",
            height=2,
            cursor="hand2"
        )
        self.start_button.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))
        
        self.stop_button = tk.Button(
            button_frame,
            text="â¹ åœæ­¢",
            command=self.stop_crawling,
            font=("å¾®è»Ÿé›…é»‘", 12, "bold"),
            bg="#D9534F",
            fg="white",
            height=2,
            state=tk.DISABLED,
            cursor="hand2"
        )
        self.stop_button.pack(side=tk.RIGHT, fill=tk.X, expand=True, padx=(5, 0))
        
        # æ—¥èªŒé¡¯ç¤º
        log_frame = tk.LabelFrame(main_frame, text="é‹è¡Œæ—¥èªŒ", font=("å¾®è»Ÿé›…é»‘", 11, "bold"), padx=10, pady=10)
        log_frame.pack(fill=tk.BOTH, expand=True)
        
        self.log_text = scrolledtext.ScrolledText(
            log_frame,
            height=12,
            font=("Consolas", 9),
            wrap=tk.WORD
        )
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # ç‰ˆæ¬Šæç¤º
        copyright_label = tk.Label(
            self.root,
            text="âš ï¸ åƒ…ä¾›å­¸ç¿’ç ”ç©¶ä½¿ç”¨ | è«‹éµå®ˆç‰ˆæ¬Šæ³•è¦ | ä¸å¾—ç”¨æ–¼å•†æ¥­ç”¨é€”",
            font=("å¾®è»Ÿé›…é»‘", 8),
            fg="red"
        )
        copyright_label.pack(pady=5)
        
        # åˆå§‹åŒ–ç•Œé¢ç‹€æ…‹
        self.toggle_type()
        self.toggle_mode()

    def _normalize_name(self, text: str) -> str:
        """åç¨±æ­£è¦åŒ–ï¼šç§»é™¤ç©ºç™½èˆ‡å¤§å°å¯«å·®ç•°ï¼Œæå‡åŒ¹é…ç©©å®šæ€§ã€‚"""
        import re as _re
        return _re.sub(r"\s+", "", (text or "").strip()).lower()

    def http_get(self, url: str, delay: float, allow_retry: bool = True):
        """å…·é‡è©¦èˆ‡é€€é¿æ©Ÿåˆ¶çš„ GETã€‚
        - å° 429/5xx/ç©ºå…§å®¹/ç–‘ä¼¼é˜²è­·é ï¼ˆCloudflareï¼‰é€²è¡Œé‡è©¦
        - ä½¿ç”¨å…±ç”¨ Sessionï¼Œä¸¦æ”¯æ´åŸºç¤æŠ–å‹•
        """
        import random as _rand
        import time as _time
        last_exc = None
        attempts = self.max_retries if allow_retry else 1
        for attempt in range(1, attempts + 1):
            try:
                if delay and attempt == 1:
                    _time.sleep(delay)
                elif attempt > 1:
                    backoff = (self.backoff_base ** (attempt - 1)) + _rand.uniform(0, self.jitter_seconds)
                    self.log(f"    é‡è©¦ç¬¬{attempt}æ¬¡ï¼Œç­‰å¾… {backoff:.1f}s ...")
                    _time.sleep(backoff)

                resp = self.session.get(url, timeout=self.request_timeout)
                text = resp.text if resp and hasattr(resp, 'text') else ''
                # åˆ¤æ–·å¯ç–‘é é¢
                suspicious = any(k in text for k in [
                    'Just a moment', 'Checking your browser', 'Cloudflare', 'æ³¨æ„å®‰å…¨', 'äººæœºéªŒè¯'
                ])
                if resp.status_code == 200 and text.strip() and not suspicious:
                    return resp
                else:
                    self.log(f"    è«‹æ±‚éé æœŸ HTTP {resp.status_code} æˆ–å…§å®¹å¯ç–‘ï¼Œæº–å‚™é‡è©¦")
            except Exception as e:  # åƒ…åœ¨ç¶²è·¯å±¤é‡è©¦
                last_exc = e
                self.log(f"    è«‹æ±‚éŒ¯èª¤: {e}")
        if last_exc:
            raise last_exc
        return None

    def _build_paged_url(self, url: str, page: int) -> str:
        """ç‚ºäººç‰©é å»ºç«‹åˆ†é  URLã€‚è‹¥åŸæœ¬å·²æœ‰ queryï¼Œè¿½åŠ  &page=ï¼Œå¦å‰‡ ?page=."""
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
        parsed = urlparse(url)
        query = dict(parse_qsl(parsed.query))
        query['page'] = str(page)
        new_qs = urlencode(query)
        return urlunparse(parsed._replace(query=new_qs))

    def _extract_max_page(self, soup: BeautifulSoup, current_url: str) -> int:
        """å¾é é¢ä¸­çš„åˆ†é é€£çµæ¨æ–·æœ€å¤§é ç¢¼ï¼Œæ‰¾ä¸åˆ°æ™‚å›å‚³ 1ã€‚"""
        try:
            anchors = soup.find_all('a', href=True)
            max_page = 1
            for a in anchors:
                href = a.get('href', '')
                if 'page=' in href:
                    # è§£ææ•´æ•¸é ç¢¼
                    import re as _re
                    m = _re.search(r'[?&]page=(\d+)', href)
                    if m:
                        max_page = max(max_page, int(m.group(1)))
            return max_page
        except Exception:
            return 1
    
    def toggle_type(self):
        """æ ¹æ“šé¸æ“‡çš„é¡å‹æ›´æ–°æç¤ºæ–‡å­—"""
        type_name = self.type_var.get()
        if type_name == "writer":
            self.name_label.config(text="ä½œè©äººå:")
            self.name_hint.config(text="(ä¾‹: æ—å¤•ã€æ–¹æ–‡å±±)")
        elif type_name == "composer":
            self.name_label.config(text="ä½œæ›²äººå:")
            self.name_hint.config(text="(ä¾‹: å‘¨æ°å€«ã€æ—ä¿Šå‚‘)")
        elif type_name == "singer":
            self.name_label.config(text="æ­Œæ‰‹å:")
            self.name_hint.config(text="(ä¾‹: å‘¨æ·±ã€è–›ä¹‹è¬™)")
    
    def toggle_mode(self):
        """æ ¹æ“šé¸æ“‡çš„æ¨¡å¼é¡¯ç¤ºå°æ‡‰çš„è¼¸å…¥æ¡†"""
        # éš±è—æ‰€æœ‰è¼¸å…¥æ¡†
        self.name_frame.pack_forget()
        self.page_frame.pack_forget()
        
        # æ ¹æ“šé¸æ“‡çš„æ¨¡å¼é¡¯ç¤ºå°æ‡‰çš„è¼¸å…¥æ¡†
        if self.mode_var.get() == "name":
            self.name_frame.pack(fill=tk.X, pady=5)
        elif self.mode_var.get() == "pages":
            self.page_frame.pack(fill=tk.X, pady=5)
    
    def select_folder(self):
        """é–‹å•Ÿè³‡æ–™å¤¾é¸æ“‡å°è©±æ¡†ï¼Œè®“ä½¿ç”¨è€…é¸æ“‡ä¿å­˜ä½ç½®"""
        folder = filedialog.askdirectory(title="é¸æ“‡ä¿å­˜ä½ç½®")
        if folder:
            self.path_var.set(folder)
            self.log(f"ğŸ“ ä¿å­˜ä½ç½®: {folder}")
    
    def log(self, message):
        """æ·»åŠ æ—¥èªŒè¨Šæ¯åˆ°é¡¯ç¤ºå€åŸŸ"""
        self.log_text.insert(tk.END, f"{message}\n")
        self.log_text.see(tk.END)
        self.root.update()
    
    def start_crawling(self):
        """é©—è­‰è¼¸å…¥åƒæ•¸ä¸¦é–‹å§‹çˆ¬å–ä½œæ¥­"""
        # é©—è­‰è¼¸å…¥
        if self.mode_var.get() == "pages":
            try:
                start_page = int(self.start_page_entry.get())
                end_page = int(self.end_page_entry.get())
                if start_page < 1 or end_page < start_page:
                    messagebox.showerror("éŒ¯èª¤", "é æ•¸ç¯„åœç„¡æ•ˆï¼")
                    return
            except ValueError:
                messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥æœ‰æ•ˆçš„é æ•¸ï¼")
                return
        elif self.mode_var.get() == "name":
            name = self.name_entry.get().strip()
            if not name:
                type_name = self.type_var.get()
                if type_name == "writer":
                    messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥ä½œè©äººåï¼")
                elif type_name == "composer":
                    messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥ä½œæ›²äººåï¼")
                elif type_name == "singer":
                    messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥æ­Œæ‰‹åï¼")
                return
        
        try:
            delay = float(self.delay_entry.get())
            if delay < 1:
                messagebox.showwarning("è­¦å‘Š", "å»ºè­°å»¶é²è‡³å°‘1ç§’ï¼Œé¿å…è¢«å°ç¦ï¼")
        except ValueError:
            messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥æœ‰æ•ˆçš„å»¶é²æ™‚é–“ï¼")
            return
        
        # ç¢ºèªé–‹å§‹
        if not messagebox.askyesno(
            "ç¢ºèª", 
            "é–‹å§‹çˆ¬å–ï¼Ÿ\n\nâš ï¸ è«‹ç¢ºä¿:\n- åƒ…ç”¨æ–¼å€‹äººå­¸ç¿’ç ”ç©¶\n- éµå®ˆç‰ˆæ¬Šæ³•è¦\n- ä¸ç”¨æ–¼å•†æ¥­ç”¨é€”"
        ):
            return
        
        # å•Ÿå‹•çˆ¬èŸ²åŸ·è¡Œç·’
        self.is_crawling = True
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.log_text.delete(1.0, tk.END)
        
        thread = threading.Thread(target=self.crawl_thread, daemon=True)
        thread.start()
    
    def stop_crawling(self):
        """åœæ­¢çˆ¬å–ä½œæ¥­"""
        self.is_crawling = False
        self.log("\nâ¹ æ­£åœ¨åœæ­¢...")
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
    
    def crawl_thread(self):
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­åŸ·è¡Œçˆ¬å–ä½œæ¥­"""
        try:
            if self.mode_var.get() == "pages":
                self.crawl_by_pages()
            elif self.mode_var.get() == "name":
                self.crawl_by_name()
        except Exception as e:
            self.log(f"\nâŒ éŒ¯èª¤: {e}")
        finally:
            self.is_crawling = False
            self.start_button.config(state=tk.NORMAL)
            self.stop_button.config(state=tk.DISABLED)
    
    def crawl_by_pages(self):
        """æ ¹æ“šæŒ‡å®šçš„é æ•¸ç¯„åœçˆ¬å–æŒ‡å®šé¡å‹çš„æ‰€æœ‰æ­Œæ›²"""
        start_page = int(self.start_page_entry.get())
        end_page = int(self.end_page_entry.get())
        delay = float(self.delay_entry.get())
        output_dir = self.path_var.get()
        type_name = self.type_var.get()
        
        type_text = {"writer": "ä½œè©äºº", "composer": "ä½œæ›²äºº", "singer": "æ­Œæ‰‹"}[type_name]
        
        self.log("="*60)
        self.log(f"ğŸš€ é–‹å§‹æŒ‰é æ•¸çˆ¬å–{type_text} (ç¬¬{start_page}-{end_page}é )")
        self.log("="*60)
        
        stats = {'people': 0, 'songs': 0}
        
        for page_num in range(start_page, end_page + 1):
            if not self.is_crawling:
                break
            
            self.log(f"\nğŸ“„ è™•ç†ç¬¬ {page_num} é ...")
            
            if type_name == "writer":
                people = self.get_writers_from_page(page_num, delay)
            elif type_name == "composer":
                people = self.get_composers_from_page(page_num, delay)
            elif type_name == "singer":
                people = self.get_singers_from_page(page_num, delay)
            
            if not people:
                continue
            
            page_dir = os.path.join(output_dir, f"page_{page_num}")
            os.makedirs(page_dir, exist_ok=True)
            
            for idx, person in enumerate(people, 1):
                if not self.is_crawling:
                    break
                
                self.log(f"  [{idx}/{len(people)}] {person['name']}")
                
                if type_name == "writer":
                    count = self.crawl_writer_songs(person, page_dir, delay)
                elif type_name == "composer":
                    count = self.crawl_composer_songs(person, page_dir, delay)
                elif type_name == "singer":
                    count = self.crawl_singer_songs(person, page_dir, delay)
                
                stats['songs'] += count
                stats['people'] += 1
        
        self.log("\n" + "="*60)
        self.log(f"âœ… å®Œæˆï¼{type_text}: {stats['people']} | æ­Œæ›²: {stats['songs']}")
        self.log("="*60)
        messagebox.showinfo("å®Œæˆ", f"çˆ¬å–å®Œæˆï¼\n\n{type_text}: {stats['people']}\næ­Œæ›²: {stats['songs']}")
    
    def crawl_by_name(self):
        """æ ¹æ“šé¸æ“‡çš„é¡å‹å’Œè¼¸å…¥çš„äººåæœå°‹ä¸¦çˆ¬å–å…¶æ‰€æœ‰æ­Œæ›²"""
        name = self.name_entry.get().strip()
        delay = float(self.delay_entry.get())
        output_dir = self.path_var.get()
        type_name = self.type_var.get()
        
        type_text = {"writer": "ä½œè©äºº", "composer": "ä½œæ›²äºº", "singer": "æ­Œæ‰‹"}[type_name]
        
        self.log("="*60)
        self.log(f"ğŸš€ é–‹å§‹çˆ¬å–{type_text}: {name}")
        self.log("="*60)
        
        # æœå°‹æŒ‡å®šé¡å‹çš„äºº
        self.log(f"\nğŸ” æœå°‹{type_text}...")
        
        if type_name == "writer":
            person_info = self.search_writer(name, delay)
        elif type_name == "composer":
            person_info = self.search_composer(name, delay)
        elif type_name == "singer":
            person_info = self.search_singer(name, delay)
        
        if not person_info:
            self.log(f"âŒ æœªæ‰¾åˆ°{type_text}: {name}")
            messagebox.showerror("éŒ¯èª¤", f"æœªæ‰¾åˆ°{type_text}: {name}")
            return
        
        self.log(f"âœ“ æ‰¾åˆ°: {person_info['name']}")
        
        person_dir = os.path.join(output_dir, self.safe_filename(person_info['name']))
        os.makedirs(person_dir, exist_ok=True)
        
        # çˆ¬å–æ­Œæ›²
        if type_name == "writer":
            count = self.crawl_writer_songs(person_info, output_dir, delay)
        elif type_name == "composer":
            count = self.crawl_composer_songs(person_info, output_dir, delay)
        elif type_name == "singer":
            count = self.crawl_singer_songs(person_info, output_dir, delay)
        
        self.log("\n" + "="*60)
        self.log(f"âœ… å®Œæˆï¼ä¸‹è¼‰æ­Œæ›²: {count}")
        self.log("="*60)
        messagebox.showinfo("å®Œæˆ", f"çˆ¬å–å®Œæˆï¼\n\nä¸‹è¼‰æ­Œæ›²: {count}")
    
    
    def get_writers_from_page(self, page_num, delay):
        """å¾æŒ‡å®šé æ•¸ç²å–æ‰€æœ‰ä½œè©äººè³‡è¨Š"""
        try:
            time.sleep(delay)
            url = f"{self.base_url}/writers?page={page_num}"
            response = self.http_get(url, delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            writers = []
            for link in soup.find_all('a', href=True):
                if '/writer/' in link.get('href', ''):
                    name = link.get_text(strip=True)
                    if name:
                        writers.append({
                            'name': name,
                            'url': urljoin(self.base_url, link['href'])
                        })
            
            return list({w['url']: w for w in writers}.values())
        except Exception as e:
            self.log(f"  âœ— éŒ¯èª¤: {e}")
            return []
    
    def get_composers_from_page(self, page_num, delay):
        """å¾æŒ‡å®šé æ•¸ç²å–æ‰€æœ‰ä½œæ›²äººè³‡è¨Š"""
        try:
            time.sleep(delay)
            url = f"{self.base_url}/composers?page={page_num}"
            self.log(f"    è«‹æ±‚URL: {url}")
            response = self.http_get(url, delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            composers = []
            links = soup.find_all('a', href=True)
            self.log(f"    æ‰¾åˆ° {len(links)} å€‹é€£çµ")
            
            for link in links:
                href = link.get('href', '')
                if '/composer/' in href:
                    name = link.get_text(strip=True)
                    if name:
                        composers.append({
                            'name': name,
                            'url': urljoin(self.base_url, href)
                        })
                        self.log(f"      ä½œæ›²äºº: {name}")
            
            self.log(f"    ç¸½å…±æ‰¾åˆ° {len(composers)} å€‹ä½œæ›²äºº")
            return list({c['url']: c for c in composers}.values())
        except Exception as e:
            self.log(f"  âœ— éŒ¯èª¤: {e}")
            return []
    
    def get_singers_from_page(self, page_num, delay):
        """å¾æŒ‡å®šé æ•¸ç²å–æ‰€æœ‰æ­Œæ‰‹è³‡è¨Š"""
        try:
            time.sleep(delay)
            url = f"{self.base_url}/singers?page={page_num}"
            response = self.http_get(url, delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            singers = []
            for link in soup.find_all('a', href=True):
                if '/singer/' in link.get('href', ''):
                    name = link.get_text(strip=True)
                    if name:
                        singers.append({
                            'name': name,
                            'url': urljoin(self.base_url, link['href'])
                        })
            
            return list({s['url']: s for s in singers}.values())
        except Exception as e:
            self.log(f"  âœ— éŒ¯èª¤: {e}")
            return []
    
    def search_writer(self, writer_name, delay):
        """æœå°‹æŒ‡å®šä½œè©äººï¼›å„ªå…ˆç²¾ç¢ºç›¸ç­‰ï¼Œå…¶æ¬¡æ¨¡ç³ŠåŒ…å«ï¼›æ”¯æ´è²¼ URL/IDã€‚"""
        import re as _re

        # æ­£è¦åŒ–è¼¸å…¥ï¼šç§»é™¤æ‰€æœ‰ç©ºç™½ï¼ˆé¿å…å…¨å½¢/åŠå½¢æˆ–å°¾éš¨ç©ºç™½é€ æˆèª¤åˆ¤ï¼‰
        text = _re.sub(r"\s+", "", writer_name.strip())

        # 1) å…ˆè™•ç† URL/ID ç›´é”
        candidate_id = None
        if text.startswith('http') and '/writer/' in text:
            try:
                candidate_id = text.split('/writer/', 1)[1].split('?')[0].strip('/ ')
            except Exception:
                candidate_id = None
        elif _re.fullmatch(r"[A-Za-z0-9]{6,12}", text):
            candidate_id = text

        if candidate_id:
            try:
                time.sleep(delay)
                url = urljoin(self.base_url, f"/writer/{candidate_id}")
                self.log(f"    ç›´æ¥ä½¿ç”¨ID/URLæŠ“å–ä½œè©äººé é¢: {url}")
                resp = self.http_get(url, delay)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'lxml')
                    full_text = soup.get_text('\n')
                    # å¾é é¢æ–‡æœ¬ä¸­è§£æã€Œä½œè©: XXXã€ï¼Œä¸¦å»é™¤ç©ºç™½æå‡æ¯”å°ç©©å®šæ€§
                    match = _re.search(r"ä½œ[è¯è©]\s*[:ï¼š]\s*([^\n\r]+)", full_text)
                    parsed_name = (_re.sub(r"\s+", "", match.group(1)) if match else "")
                    return {'name': parsed_name or writer_name, 'url': url}
                else:
                    self.log(f"    âœ— ç›´æ¥è«‹æ±‚å¤±æ•— HTTP {resp.status_code}")
            except Exception as _e:
                self.log(f"    âœ— ç›´æ¥è«‹æ±‚éŒ¯èª¤: {_e}")

        # 2) å›é€€ï¼šéæ­·å‰ 15 é ï¼›å…ˆæ‰¾ç²¾ç¢ºç›¸ç­‰ï¼Œå†æ‰¾åŒ…å«åŒ¹é…
        exact_hit = None
        fuzzy_hit = None
        for page in range(1, 16):
            try:
                time.sleep(delay)
                writers = self.get_writers_from_page(page, 0)
                for writer in writers:
                    wname = _re.sub(r"\s+", "", writer['name'])
                    if wname == text and exact_hit is None:
                        exact_hit = writer
                    if (text in wname or wname in text) and fuzzy_hit is None:
                        fuzzy_hit = writer
                if exact_hit:
                    break
            except Exception:
                continue
        return exact_hit or fuzzy_hit
    
    def search_composer(self, composer_name, delay):
        """åœ¨å‰5é ä¸­æœå°‹æŒ‡å®šçš„ä½œæ›²äºº"""
        # éæ­·å‰5é æŸ¥æ‰¾ä½œæ›²äºº
        for page in range(1, 6):
            time.sleep(delay)
            self.log(f"    æœå°‹ç¬¬{page}é ...")
            composers = self.get_composers_from_page(page, 0)
            self.log(f"    æ‰¾åˆ° {len(composers)} å€‹ä½œæ›²äºº")
            
            for composer in composers:
                self.log(f"    æª¢æŸ¥: {composer['name']}")
                if composer_name in composer['name'] or composer['name'] in composer_name:
                    self.log(f"    âœ“ åŒ¹é…: {composer['name']}")
                    return composer
        return None
    
    def search_singer(self, singer_name, delay):
        """åœ¨å‰5é ä¸­æœå°‹æŒ‡å®šçš„æ­Œæ‰‹"""
        # éæ­·å‰5é æŸ¥æ‰¾æ­Œæ‰‹
        for page in range(1, 6):
            time.sleep(delay)
            singers = self.get_singers_from_page(page, 0)
            for singer in singers:
                if singer_name in singer['name'] or singer['name'] in singer_name:
                    return singer
        return None
    
    def crawl_writer_songs(self, writer_info, base_dir, delay):
        """çˆ¬å–æŒ‡å®šä½œè©äººçš„æ‰€æœ‰æ­Œæ›²ä¸¦ä¿å­˜åˆ°æª”æ¡ˆ"""
        try:
            time.sleep(delay)
            response = self.http_get(writer_info['url'], delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            # è§£ææœ€å¤§åˆ†é 
            max_page = self._extract_max_page(soup, writer_info['url'])
            songs_map = {}
            
            def parse_song_table(table_soup):
                table = table_soup.find('table', {'class': 'table'}) or table_soup.find('table')
                if not table:
                    return
                rows = table.find_all('tr')[1:]
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        link = cells[1].find('a') or cells[0].find('a')
                        if not link:
                            continue
                        href = link.get('href', '')
                        song_url = href if href.startswith('http') else urljoin(self.base_url, href)
                        song_name = link.get_text(strip=True)
                        singer_name = cells[2].get_text(strip=True) if len(cells) >= 3 else ''
                        songs_map[song_url] = {
                            'name': song_name,
                            'singer': singer_name or 'æœªçŸ¥æ­Œæ‰‹',
                            'url': song_url,
                        }

            parse_song_table(soup)
            if max_page > 1:
                for p in range(2, max_page + 1):
                    if not self.is_crawling:
                        break
                    page_url = self._build_paged_url(writer_info['url'], p)
                    resp_p = self.http_get(page_url, delay)
                    parse_song_table(BeautifulSoup(resp_p.text, 'lxml'))
            songs = list(songs_map.values())
            
            self.log(f"    æ‰¾åˆ° {len(songs)} é¦–æ­Œæ›²")
            
            writer_dir = os.path.join(base_dir, self.safe_filename(writer_info['name']))
            os.makedirs(writer_dir, exist_ok=True)
            
            count = 0
            for song in songs:
                if not self.is_crawling:
                    break
                
                filename = f"{self.safe_filename(song['singer'])} - {self.safe_filename(song['name'])}.txt"
                filepath = os.path.join(writer_dir, filename)
                
                if os.path.exists(filepath):
                    continue
                
                lyrics = self.download_lyrics(song['url'], delay)
                if lyrics:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(lyrics)
                    count += 1
            
            return count
            
        except Exception as e:
            self.log(f"    âœ— éŒ¯èª¤: {e}")
            # è¼¸å‡ºèª¿è©¦å¿«ç…§
            try:
                debug_dir = os.path.join(base_dir, '_debug')
                os.makedirs(debug_dir, exist_ok=True)
                with open(os.path.join(debug_dir, 'writer_error.html'), 'w', encoding='utf-8') as f:
                    f.write(response.text if 'response' in locals() and hasattr(response, 'text') else '')
            except Exception:
                pass
            return 0
    
    def crawl_composer_songs(self, composer_info, base_dir, delay):
        """çˆ¬å–æŒ‡å®šä½œæ›²äººçš„æ‰€æœ‰æ­Œæ›²ä¸¦ä¿å­˜åˆ°æª”æ¡ˆ"""
        try:
            time.sleep(delay)
            response = self.http_get(composer_info['url'], delay)
            soup = BeautifulSoup(response.text, 'lxml')

            max_page = self._extract_max_page(soup, composer_info['url'])
            songs_map = {}

            def parse_table(table_soup):
                table = table_soup.find('table', {'class': 'table'}) or table_soup.find('table')
                if not table:
                    return
                rows = table.find_all('tr')[1:]
                self.log(f"    è§£æåˆ° {len(rows)} è¡Œè³‡æ–™")
                for row in rows:
                    cells = row.find_all('td')
                    link = None
                    for cell in cells:
                        a = cell.find('a', href=True)
                        if a and ('/song/' in a.get('href','') or '/lyric/' in a.get('href','')):
                            link = a
                            break
                    if not link and len(cells) >= 2:
                        a = cells[1].find('a', href=True)
                        if a:
                            link = a
                    if not link:
                        continue
                    href = link.get('href','')
                    song_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    song_name = link.get_text(strip=True)
                    singer_name = cells[2].get_text(strip=True) if len(cells) >= 3 else ''
                    songs_map[song_url] = {
                        'name': song_name,
                        'singer': singer_name or 'æœªçŸ¥æ­Œæ‰‹',
                        'url': song_url,
                    }

            parse_table(soup)
            if max_page > 1:
                for p in range(2, max_page + 1):
                    if not self.is_crawling:
                        break
                    page_url = self._build_paged_url(composer_info['url'], p)
                    resp_p = self.http_get(page_url, delay)
                    parse_table(BeautifulSoup(resp_p.text, 'lxml'))

            songs = list(songs_map.values())
            
            self.log(f"    ç¸½å…±æ‰¾åˆ° {len(songs)} é¦–æ­Œæ›²")
            
            composer_dir = os.path.join(base_dir, self.safe_filename(composer_info['name']))
            os.makedirs(composer_dir, exist_ok=True)
            
            count = 0
            for song in songs:
                if not self.is_crawling:
                    break
                
                filename = f"{self.safe_filename(song['singer'])} - {self.safe_filename(song['name'])}.txt"
                filepath = os.path.join(composer_dir, filename)
                
                if os.path.exists(filepath):
                    continue
                
                lyrics = self.download_lyrics(song['url'], delay)
                if lyrics:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(lyrics)
                    count += 1
                    self.log(f"      ä¸‹è¼‰: {song['name']}")
                else:
                    self.log(f"      âœ— ç„¡æ³•ä¸‹è¼‰: {song['name']}")
            
            return count
            
        except Exception as e:
            self.log(f"    âœ— é”™è¯¯: {e}")
            try:
                debug_dir = os.path.join(base_dir, '_debug')
                os.makedirs(debug_dir, exist_ok=True)
                with open(os.path.join(debug_dir, 'composer_error.html'), 'w', encoding='utf-8') as f:
                    f.write(response.text if 'response' in locals() and hasattr(response, 'text') else '')
            except Exception:
                pass
            return 0
    
    def crawl_singer_songs(self, singer_info, base_dir, delay):
        """çˆ¬å–æŒ‡å®šæ­Œæ‰‹çš„æ‰€æœ‰æ­Œæ›²ä¸¦ä¿å­˜åˆ°æª”æ¡ˆ"""
        try:
            time.sleep(delay)
            response = self.http_get(singer_info['url'], delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            # è§£ææœ€å¤§åˆ†é ä¸¦æ”¶é›†æ‰€æœ‰æ­Œæ›²
            max_page = self._extract_max_page(soup, singer_info['url'])
            songs_map = {}

            def parse_table(table_soup):
                table = table_soup.find('table', {'class': 'table'}) or table_soup.find('table')
                if not table:
                    return
                rows = table.find_all('tr')[1:]
                self.log(f"    è§£æåˆ° {len(rows)} è¡Œè³‡æ–™")
                for row in rows:
                    cells = row.find_all('td')
                    link = None
                    for cell in cells:
                        a = cell.find('a', href=True)
                        if a and ('/song/' in a.get('href','') or '/lyric/' in a.get('href','')):
                            link = a
                            break
                    if not link and len(cells) >= 2:
                        a = cells[1].find('a', href=True)
                        if a:
                            link = a
                    if not link:
                        continue
                    href = link.get('href','')
                    song_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    song_name = link.get_text(strip=True)
                    singer_name = cells[2].get_text(strip=True) if len(cells) >= 3 else singer_info['name']
                    songs_map[song_url] = {
                        'name': song_name,
                        'singer': singer_name or singer_info['name'],
                        'url': song_url,
                    }

            parse_table(soup)
            if max_page > 1:
                for p in range(2, max_page + 1):
                    if not self.is_crawling:
                        break
                    page_url = self._build_paged_url(singer_info['url'], p)
                    resp_p = self.http_get(page_url, delay)
                    parse_table(BeautifulSoup(resp_p.text, 'lxml'))

            songs = list(songs_map.values())
            
            self.log(f"    ç¸½å…±æ‰¾åˆ° {len(songs)} é¦–æ­Œæ›²")
            
            singer_dir = os.path.join(base_dir, self.safe_filename(singer_info['name']))
            os.makedirs(singer_dir, exist_ok=True)
            
            count = 0
            for song in songs:
                if not self.is_crawling:
                    break
                
                filename = f"{self.safe_filename(song['singer'])} - {self.safe_filename(song['name'])}.txt"
                filepath = os.path.join(singer_dir, filename)
                
                if os.path.exists(filepath):
                    continue
                
                lyrics = self.download_lyrics(song['url'], delay)
                if lyrics:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(lyrics)
                    count += 1
            
            return count
            
        except Exception as e:
            self.log(f"    âœ— é”™è¯¯: {e}")
            try:
                debug_dir = os.path.join(base_dir, '_debug')
                os.makedirs(debug_dir, exist_ok=True)
                with open(os.path.join(debug_dir, 'singer_error.html'), 'w', encoding='utf-8') as f:
                    f.write(response.text if 'response' in locals() and hasattr(response, 'text') else '')
            except Exception:
                pass
            return 0
    
    def download_lyrics(self, url, delay):
        """å¾æŒ‡å®šURLä¸‹è¼‰æ­Œè©å…§å®¹ã€‚

        é¦–é¸ï¼šæå–ã€Œä¸‹è¼‰txtæ–‡æª”ã€åˆ°ã€Œæ›´å¤šã€ä¹‹é–“çš„å…§å®¹ã€‚
        å›é€€ï¼šè‹¥æ‰¾ä¸åˆ°æ¨™è¨˜ï¼Œå‰‡ã€Œä¸‹è¼‰ä½†ä¸è½‰åŒ–ã€â€”â€”ä¿å­˜æ•´é ç´”æ–‡å­—å…§å®¹ï¼ˆå·²å»ç©ºè¡Œï¼‰ã€‚
        """
        try:
            time.sleep(delay)
            response = self.http_get(url, delay)
            soup = BeautifulSoup(response.text, 'lxml')
            
            # å„ªå…ˆå˜—è©¦å¸¸è¦‹æ­Œè©å®¹å™¨çš„ç²¾ç¢ºæ“·å–
            container = None
            for sel in ['#lyrics', '.lyrics', '.lyric', 'article', 'pre.lyrics', 'pre']:
                node = soup.select_one(sel)
                if node and node.get_text(strip=True):
                    container = node
                    break
            if container is not None:
                raw = container.get_text('\n')
                clean = [ln.strip() for ln in raw.split('\n') if ln.strip()]
                if len(clean) >= 3:
                    return '\n'.join(clean)

            # å›é€€åˆ°å…¨æ–‡æ¨™è¨˜æ“·å–
            text = soup.get_text()
            lines = text.split('\n')
            
            # æŸ¥æ‰¾æ¨™è¨˜ä½ç½®
            start_idx = None
            end_idx = None
            
            for i, line in enumerate(lines):
                line = line.strip()
                # å°‹æ‰¾é–‹å§‹æ¨™è¨˜ï¼šä¸‹è¼‰txtæ–‡æª”
                if ('ä¸‹è¼‰txtæ–‡æª”' in line or 'txt æ–‡æª”' in line or 'ä¸‹è½½txtæ–‡æ¡£' in line or 'ä¸‹è¼‰TXTæ–‡æª”' in line or 'ä¸‹è½½TXTæ–‡æ¡£' in line):
                    start_idx = i + 1
                    self.log(f"      æ‰¾åˆ°é–‹å§‹æ¨™è¨˜: {line}")
                # å°‹æ‰¾çµæŸæ¨™è¨˜ï¼šæ›´å¤š
                elif (('æ›´å¤š' in line) or ('æ›´å¤šå†…å®¹' in line) or ('æ›´å¤šæ­Œæ›²' in line)) and start_idx is not None:
                    end_idx = i
                    self.log(f"      æ‰¾åˆ°çµæŸæ¨™è¨˜: {line}")
                    break
            
            if start_idx and end_idx and start_idx < end_idx:
                extracted = lines[start_idx:end_idx]
                clean_lines = [line.strip() for line in extracted if line.strip()]
                self.log(f"      æå–äº† {len(clean_lines)} è¡Œæ­Œè©")
                return '\n'.join(clean_lines)
            else:
                # Fallbackï¼šç„¡æ¨™è¨˜æ™‚ï¼Œç›´æ¥ä¿å­˜æ•´é ç´”æ–‡å­—
                self.log(f"      æœªæ‰¾åˆ°æ¨™è¨˜ï¼Œæ”¹ç‚ºä¿å­˜åŸå§‹é é¢æ–‡å­—ï¼ˆä¸è½‰åŒ–ï¼‰")
                clean_full = [ln.strip() for ln in lines if ln.strip()]
                return '\n'.join(clean_full)
            
        except Exception as e:
            self.log(f"      ä¸‹è¼‰æ­Œè©éŒ¯èª¤: {e}")
            return None
    
    def safe_filename(self, name):
        """å°‡æª”æ¡ˆåè½‰æ›ç‚ºå®‰å…¨çš„æª”æ¡ˆç³»çµ±åç¨±"""
        return re.sub(r'[<>:"/\\|?*]', '_', name)[:100]


def main():
    """ä¸»ç¨‹å¼å…¥å£é»"""
    root = tk.Tk()
    LyricsCrawlerApp(root)
    root.mainloop()


if __name__ == '__main__':
    main()

