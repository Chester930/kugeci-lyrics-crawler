"""
é…·æ­Œè¯çˆ¬è™« - ä¿®å¤ç‰ˆ
æ­£ç¡®æå–ä½œè¯äººçš„æ­Œæ›²åˆ—è¡¨ï¼ˆä»è¡¨æ ¼ä¸­è·å–ï¼‰
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin
import re

class KugeciCrawlerFixed:
    def __init__(self, output_dir='lyrics_data_fixed', start_page=1, end_page=3, delay=1.5):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param output_dir: è¾“å‡ºç›®å½•
        :param start_page: èµ·å§‹é¡µç 
        :param end_page: ç»“æŸé¡µç 
        :param delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.base_url = "https://www.kugeci.com"
        self.output_dir = output_dir
        self.start_page = start_page
        self.end_page = end_page
        self.delay = delay
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'writers_processed': 0,
            'songs_downloaded': 0,
            'errors': [],
            'writer_details': []  # è®°å½•æ¯ä¸ªä½œè¯äººçš„æ­Œæ›²æ•°
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
    
    def safe_filename(self, name):
        """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        return name[:100] if len(name) > 100 else name
    
    def get_writers_from_page(self, page_num):
        """
        è·å–æŸä¸€é¡µçš„æ‰€æœ‰ä½œè¯äºº
        """
        url = f"{self.base_url}/writers?page={page_num}"
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è·å–ç¬¬ {page_num} é¡µä½œè¯äººåˆ—è¡¨...")
        print(f"URL: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            writers = []
            # æŸ¥æ‰¾ä½œè¯äººé“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if '/writer/' in href:
                    name = link.get_text(strip=True)
                    if name:
                        writer_url = urljoin(self.base_url, href)
                        writers.append({
                            'name': name,
                            'url': writer_url,
                            'id': href.split('/')[-1]
                        })
            
            # å»é‡
            unique_writers = []
            seen_ids = set()
            for writer in writers:
                if writer['id'] not in seen_ids:
                    unique_writers.append(writer)
                    seen_ids.add(writer['id'])
            
            print(f"âœ“ æ‰¾åˆ° {len(unique_writers)} ä¸ªä½œè¯äºº")
            return unique_writers
            
        except Exception as e:
            print(f"âœ— è·å–ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
            self.stats['errors'].append(f"Page {page_num}: {e}")
            return []
    
    def get_songs_from_writer(self, writer_url, writer_name):
        """
        ã€ä¿®å¤ã€‘ä»è¡¨æ ¼ä¸­è·å–ä½œè¯äººçš„æ­Œæ›²åˆ—è¡¨
        """
        try:
            time.sleep(self.delay)
            response = self.session.get(writer_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            songs = []
            
            # ã€å…³é”®ä¿®å¤ã€‘åªä»è¡¨æ ¼ä¸­æå–æ­Œæ›²
            table = soup.find('table', {'class': 'table'}) or soup.find('table', {'id': 'tablesort'})
            
            if table:
                # éå†è¡¨æ ¼è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
                rows = table.find_all('tr')[1:]  # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆè¡¨å¤´ï¼‰
                
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 4:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                        # åˆ—ç»“æ„ï¼šæ”¶å½•æ—¥æœŸã€æ­Œåã€æ­Œæ‰‹ã€ä½œæ›²ã€ç‚¹å‡»
                        song_link = cells[1].find('a')  # æ­Œååœ¨ç¬¬2åˆ—
                        if song_link and '/song/' in song_link.get('href', ''):
                            song_title = song_link.get_text(strip=True)
                            singer = cells[2].get_text(strip=True)  # æ­Œæ‰‹
                            composer = cells[3].get_text(strip=True)  # ä½œæ›²
                            
                            # ç»„åˆå®Œæ•´æ ‡é¢˜ï¼šæ­Œæ‰‹ - æ­Œå
                            full_title = f"{singer} - {song_title}" if singer else song_title
                            
                            songs.append({
                                'title': full_title,
                                'song_name': song_title,
                                'singer': singer,
                                'composer': composer,
                                'url': urljoin(self.base_url, song_link['href'])
                            })
            else:
                print(f"  âš ï¸  æœªæ‰¾åˆ°æ­Œæ›²è¡¨æ ¼")
            
            return songs
            
        except Exception as e:
            print(f"  âœ— è·å–æ­Œæ›²åˆ—è¡¨å¤±è´¥: {e}")
            self.stats['errors'].append(f"{writer_name}: {e}")
            return []
    
    def download_lyrics_txt(self, song_url):
        """
        ä»æ­Œæ›²é¡µé¢æå–æ­Œè¯æ–‡æœ¬
        """
        try:
            time.sleep(self.delay)
            response = self.session.get(song_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            lyrics_text = ""
            
            # æŸ¥æ‰¾åŒ…å«LRCæ ¼å¼çš„æ–‡æœ¬å—
            for elem in soup.find_all(['div', 'pre', 'p']):
                text = elem.get_text(strip=False)
                if '[00:' in text or '[01:' in text:
                    if len(text) > len(lyrics_text):
                        lyrics_text = text
            
            # æ¸…ç†æ­Œè¯æ–‡æœ¬
            if lyrics_text:
                lines = lyrics_text.split('\n')
                clean_lines = []
                for line in lines:
                    # ç§»é™¤æ—¶é—´è½´ [00:00.00]
                    clean_line = re.sub(r'\[\d{2}:\d{2}\.\d{2,3}\]', '', line).strip()
                    if clean_line:
                        clean_lines.append(clean_line)
                
                return '\n'.join(clean_lines)
            
            return None
            
        except Exception as e:
            print(f"    âœ— ä¸‹è½½æ­Œè¯å¤±è´¥: {e}")
            return None
    
    def crawl(self):
        """ä¸»çˆ¬å–å‡½æ•°"""
        print("\n" + "="*60)
        print("ğŸµ é…·æ­Œè¯çˆ¬è™«å¯åŠ¨ï¼ˆä¿®å¤ç‰ˆï¼‰")
        print("="*60)
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"é¡µç èŒƒå›´: {self.start_page} - {self.end_page}")
        print(f"è¯·æ±‚å»¶è¿Ÿ: {self.delay} ç§’")
        print("="*60)
        
        # éå†æ¯ä¸€é¡µ
        for page_num in range(self.start_page, self.end_page + 1):
            page_dir = os.path.join(self.output_dir, f"page_{page_num}")
            os.makedirs(page_dir, exist_ok=True)
            
            # è·å–è¯¥é¡µçš„æ‰€æœ‰ä½œè¯äºº
            writers = self.get_writers_from_page(page_num)
            
            if not writers:
                print(f"ç¬¬ {page_num} é¡µæ— ä½œè¯äººæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # éå†æ¯ä¸ªä½œè¯äºº
            for idx, writer in enumerate(writers, 1):
                writer_name = self.safe_filename(writer['name'])
                writer_dir = os.path.join(page_dir, writer_name)
                os.makedirs(writer_dir, exist_ok=True)
                
                print(f"\n[{idx}/{len(writers)}] å¤„ç†ä½œè¯äºº: {writer['name']}")
                print(f"  URL: {writer['url']}")
                
                # ã€ä¿®å¤ã€‘ä»è¡¨æ ¼ä¸­è·å–æ­Œæ›²åˆ—è¡¨
                songs = self.get_songs_from_writer(writer['url'], writer['name'])
                print(f"  æ‰¾åˆ° {len(songs)} é¦–æ­Œæ›²")
                
                # è®°å½•ä½œè¯äººä¿¡æ¯
                writer_info = {
                    'name': writer['name'],
                    'song_count': len(songs),
                    'songs': [song['title'] for song in songs]
                }
                self.stats['writer_details'].append(writer_info)
                
                if len(songs) == 0:
                    print(f"  âš ï¸  è¯¥ä½œè¯äººæš‚æ— æ”¶å½•æ­Œæ›²")
                    continue
                
                # ä¸‹è½½æ¯é¦–æ­Œçš„æ­Œè¯
                for song_idx, song in enumerate(songs, 1):
                    song_title = self.safe_filename(song['title'])
                    lyrics_file = os.path.join(writer_dir, f"{song_title}.txt")
                    
                    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
                    if os.path.exists(lyrics_file):
                        print(f"    [{song_idx}/{len(songs)}] {song['title']} - å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        continue
                    
                    print(f"    [{song_idx}/{len(songs)}] ä¸‹è½½: {song['title']}")
                    
                    # ä¸‹è½½æ­Œè¯
                    lyrics = self.download_lyrics_txt(song['url'])
                    
                    if lyrics:
                        # æ·»åŠ å…ƒæ•°æ®
                        metadata = f"æ­Œå: {song['song_name']}\n"
                        metadata += f"æ¼”å”±: {song['singer']}\n"
                        metadata += f"ä½œè¯: {writer['name']}\n"
                        metadata += f"ä½œæ›²: {song['composer']}\n"
                        metadata += f"\n{'='*40}\n\n"
                        
                        full_content = metadata + lyrics
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        with open(lyrics_file, 'w', encoding='utf-8') as f:
                            f.write(full_content)
                        print(f"      âœ“ å·²ä¿å­˜")
                        self.stats['songs_downloaded'] += 1
                    else:
                        print(f"      âœ— æ— æ³•è·å–æ­Œè¯")
                        self.stats['errors'].append(f"{writer['name']} - {song['title']}")
                
                self.stats['writers_processed'] += 1
                
                # ä¿å­˜è¿›åº¦
                self.save_stats()
        
        # æœ€ç»ˆç»Ÿè®¡
        self.print_summary()
    
    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = os.path.join(self.output_dir, 'crawl_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
    
    def print_summary(self):
        """æ‰“å°çˆ¬å–æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("="*60)
        print(f"å¤„ç†ä½œè¯äººæ•°: {self.stats['writers_processed']}")
        print(f"ä¸‹è½½æ­Œæ›²æ•°: {self.stats['songs_downloaded']}")
        print(f"é”™è¯¯æ•°: {len(self.stats['errors'])}")
        
        # æ˜¾ç¤ºä½œè¯äººæ­Œæ›²ç»Ÿè®¡
        if self.stats['writer_details']:
            print("\nä½œè¯äººæ­Œæ›²ç»Ÿè®¡ï¼ˆå‰10ä½ï¼‰:")
            for detail in self.stats['writer_details'][:10]:
                print(f"  {detail['name']}: {detail['song_count']}é¦–")
        
        if self.stats['errors']:
            print(f"\nå‰10ä¸ªé”™è¯¯:")
            for error in self.stats['errors'][:10]:
                print(f"  - {error}")
        print("="*60)


if __name__ == '__main__':
    # é…ç½®å‚æ•°
    crawler = KugeciCrawlerFixed(
        output_dir='lyrics_data_fixed',
        start_page=1,
        end_page=2,  # æµ‹è¯•ç”¨
        delay=1.5
    )
    
    # å¼€å§‹çˆ¬å–
    crawler.crawl()

