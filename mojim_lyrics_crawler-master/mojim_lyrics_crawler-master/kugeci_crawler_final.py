"""
é…·æ­Œè¯çˆ¬è™« - æœ€ç»ˆä¼˜åŒ–ç‰ˆ
- å»é™¤é‡å¤å…ƒæ•°æ®
- å»é™¤æ¼”å”±ä¼šå¯¹è¯
- æ·»åŠ å®Œæ•´çš„æ­Œæ›²ä¿¡æ¯å¤´
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin
import re

class KugeciCrawlerFinal:
    def __init__(self, output_dir='lyrics_final', start_page=1, end_page=3, delay=2.0):
        self.base_url = "https://www.kugeci.com"
        self.output_dir = output_dir
        self.start_page = start_page
        self.end_page = end_page
        self.delay = delay
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.stats = {
            'writers_processed': 0,
            'songs_downloaded': 0,
            'errors': []
        }
        
        os.makedirs(output_dir, exist_ok=True)
    
    def safe_filename(self, name):
        return re.sub(r'[<>:"/\\|?*]', '_', name)[:100]
    
    def extract_metadata(self, raw_text):
        """æå–å…ƒæ•°æ®ï¼ˆä½œè¯ã€ä½œæ›²ã€ç¼–æ›²ç­‰ï¼‰"""
        metadata = {}
        lines = raw_text.split('\n')
        
        for line in lines[:30]:  # åªåœ¨å‰30è¡ŒæŸ¥æ‰¾å…ƒæ•°æ®
            line = line.strip()
            # åŒ¹é… "ä½œè¯ï¼šXXX" æˆ– "ä½œè¯ : XXX" æˆ– "è¯ï¼šXXX"
            if re.match(r'(ä½œè¯|è¯)\s*[:ï¼š]\s*(.+)', line):
                match = re.match(r'(ä½œè¯|è¯)\s*[:ï¼š]\s*(.+)', line)
                if 'ä½œè¯' not in metadata:  # é¿å…é‡å¤
                    metadata['ä½œè¯'] = match.group(2).strip()
            elif re.match(r'(ä½œæ›²|æ›²)\s*[:ï¼š]\s*(.+)', line):
                match = re.match(r'(ä½œæ›²|æ›²)\s*[:ï¼š]\s*(.+)', line)
                if 'ä½œæ›²' not in metadata:
                    metadata['ä½œæ›²'] = match.group(2).strip()
            elif re.match(r'ç¼–æ›²\s*[:ï¼š]\s*(.+)', line):
                match = re.match(r'ç¼–æ›²\s*[:ï¼š]\s*(.+)', line)
                if 'ç¼–æ›²' not in metadata:
                    metadata['ç¼–æ›²'] = match.group(1).strip()
            elif re.match(r'åæœŸ\s*[:ï¼š]\s*(.+)', line):
                match = re.match(r'åæœŸ\s*[:ï¼š]\s*(.+)', line)
                if 'åæœŸ' not in metadata:
                    metadata['åæœŸ'] = match.group(1).strip()
        
        return metadata
    
    def clean_lyrics(self, raw_text):
        """æ™ºèƒ½æ¸…ç†æ­Œè¯ - åªä¿ç•™çº¯æ­Œè¯"""
        lines = raw_text.split('\n')
        lyrics_lines = []
        
        # è·³è¿‡çš„å…³é”®å­—ï¼ˆç½‘é¡µå…ƒç´ ï¼‰
        skip_keywords = [
            'English Version', 'æ—¥æœ¬èªç‰ˆ', 'é¦–é¡µ', 'ä»Šæ—¥çƒ­é—¨', 'çƒ­é—¨æ­Œæ›²', 
            'æœ€æ–°æ­Œæ›²', 'lrc/lyrics', 'txt æ–‡æ¡£', 'ä¸‹è½½', 'æ›´å¤š', 
            'æ­Œæ‰‹æœ€æ–°æ­Œæ›²', 'æ­Œæ‰‹çƒ­é—¨æ­Œæ›²', 'æœ€è¿‘30å¤©', 'ç‚¹å‡»:', 'æ”¶å½•:',
            'Get it on', 'Google Play', 'Copyright', 'è”ç³»æˆ‘ä»¬', 
            'Privacy Policy', 'æ¼”å”±ï¼š', 'æ­Œæ‰‹ï¼š'
        ]
        
        # è·³è¿‡çš„æ¨¡å¼ï¼ˆæ¼”å”±ä¼šå¯¹è¯ç­‰ï¼‰
        skip_patterns = [
            r'è§‚ä¼—[ï¼š:]',  # è§‚ä¼—ï¼šé“ä»”ï¼
            r'å“¥å“¥[ï¼š:]',  # å“¥å“¥ï¼šå“ˆå“ˆ
            r'æˆ‘ä»Šå¤©.*å¼€å¿ƒ',  # æ¼”å”±ä¼šå¯¹è¯å¼€å¤´
            r'ä¸æ˜¯å› ä¸º',
            r'å…¶å®å¥½å¤š',
            r'å¤šè°¢ä½ åœ°',
        ]
        
        in_lyrics = False
        
        for line in lines:
            # ç§»é™¤æ—¶é—´è½´
            clean_line = re.sub(r'\[\d{2}:\d{2}\.\d{2,3}\]', '', line).strip()
            
            if not clean_line:
                continue
            
            # è·³è¿‡å…ƒæ•°æ®è¡Œ
            if re.match(r'(ä½œè¯|è¯|ä½œæ›²|æ›²|ç¼–æ›²|åæœŸ|ç›‘åˆ¶|åˆ¶ä½œ)\s*[:ï¼š]', clean_line):
                continue
            
            # è·³è¿‡åŒ…å«å…³é”®å­—çš„è¡Œ
            if any(keyword in clean_line for keyword in skip_keywords):
                continue
            
            # è·³è¿‡åŒ¹é…ç‰¹å®šæ¨¡å¼çš„è¡Œ
            if any(re.search(pattern, clean_line) for pattern in skip_patterns):
                break  # é‡åˆ°æ¼”å”±ä¼šå¯¹è¯ï¼Œåœæ­¢å¤„ç†
            
            # è·³è¿‡çº¯æ•°å­—æˆ–å•å­—ç¬¦
            if clean_line.isdigit() or len(clean_line) == 1:
                continue
            
            # ä¿ç•™æ­Œè¯è¡Œ
            if len(clean_line) >= 2:
                lyrics_lines.append(clean_line)
                in_lyrics = True
        
        return '\n'.join(lyrics_lines)
    
    def format_lyrics(self, song_name, singer, writer_name, metadata, lyrics):
        """æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º"""
        output = []
        output.append(f"æ­Œåï¼š{song_name}")
        output.append(f"æ¼”å”±ï¼š{singer}")
        output.append(f"ä½œè¯ï¼š{writer_name}")
        
        # æ·»åŠ å…¶ä»–å…ƒæ•°æ®
        if 'ä½œæ›²' in metadata:
            output.append(f"ä½œæ›²ï¼š{metadata['ä½œæ›²']}")
        if 'ç¼–æ›²' in metadata:
            output.append(f"ç¼–æ›²ï¼š{metadata['ç¼–æ›²']}")
        if 'åæœŸ' in metadata:
            output.append(f"åæœŸï¼š{metadata['åæœŸ']}")
        
        output.append('')
        output.append('='*40)
        output.append('')
        output.append(lyrics)
        
        return '\n'.join(output)
    
    def get_writers_from_page(self, page_num):
        url = f"{self.base_url}/writers?page={page_num}"
        print(f"\n{'='*60}")
        print(f"è·å–ç¬¬ {page_num} é¡µä½œè¯äºº...")
        
        try:
            time.sleep(self.delay)
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            writers = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if '/writer/' in href:
                    name = link.get_text(strip=True)
                    if name:
                        writers.append({
                            'name': name,
                            'url': urljoin(self.base_url, href),
                            'id': href.split('/')[-1]
                        })
            
            unique_writers = list({w['id']: w for w in writers}.values())
            print(f"âœ“ æ‰¾åˆ° {len(unique_writers)} ä¸ªä½œè¯äºº")
            return unique_writers
            
        except Exception as e:
            print(f"âœ— å¤±è´¥: {e}")
            return []
    
    def get_songs_from_writer(self, writer_url):
        try:
            time.sleep(self.delay)
            response = self.session.get(writer_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            songs = []
            table = soup.find('table', {'class': 'table'}) or soup.find('table', {'id': 'tablesort'})
            
            if table:
                rows = table.find_all('tr')[1:]
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 4:
                        song_link = cells[1].find('a')
                        if song_link:
                            songs.append({
                                'song_name': song_link.get_text(strip=True),
                                'singer': cells[2].get_text(strip=True),
                                'url': urljoin(self.base_url, song_link['href'])
                            })
            
            return songs
            
        except Exception as e:
            return []
    
    def download_and_process_lyrics(self, song_url, song_name, singer, writer_name):
        try:
            time.sleep(self.delay)
            response = self.session.get(song_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–åŸå§‹æ­Œè¯
            raw_lyrics = ""
            for elem in soup.find_all(['div', 'pre']):
                text = elem.get_text(strip=False)
                if '[00:' in text and len(text) > 100:
                    if len(text) > len(raw_lyrics):
                        raw_lyrics = text
            
            if raw_lyrics:
                # æå–å…ƒæ•°æ®
                metadata = self.extract_metadata(raw_lyrics)
                
                # æ¸…ç†æ­Œè¯
                clean = self.clean_lyrics(raw_lyrics)
                
                # æ ¼å¼åŒ–è¾“å‡º
                final_output = self.format_lyrics(
                    song_name, singer, writer_name, metadata, clean
                )
                
                return final_output
            
            return None
            
        except Exception as e:
            return None
    
    def crawl(self):
        print(f"\n{'='*60}")
        print("ğŸµ é…·æ­Œè¯çˆ¬è™« - æœ€ç»ˆä¼˜åŒ–ç‰ˆ")
        print(f"{'='*60}")
        
        for page_num in range(self.start_page, self.end_page + 1):
            page_dir = os.path.join(self.output_dir, f"page_{page_num}")
            os.makedirs(page_dir, exist_ok=True)
            
            writers = self.get_writers_from_page(page_num)
            if not writers:
                continue
            
            for idx, writer in enumerate(writers, 1):
                writer_name = self.safe_filename(writer['name'])
                writer_dir = os.path.join(page_dir, writer_name)
                os.makedirs(writer_dir, exist_ok=True)
                
                print(f"\n[{idx}/{len(writers)}] {writer['name']}")
                
                songs = self.get_songs_from_writer(writer['url'])
                print(f"  æ­Œæ›²: {len(songs)}é¦–")
                
                for song_idx, song in enumerate(songs, 1):
                    full_title = f"{song['singer']} - {song['song_name']}"
                    safe_title = self.safe_filename(full_title)
                    lyrics_file = os.path.join(writer_dir, f"{safe_title}.txt")
                    
                    if os.path.exists(lyrics_file):
                        print(f"    [{song_idx}/{len(songs)}] {full_title} - å·²å­˜åœ¨")
                        continue
                    
                    print(f"    [{song_idx}/{len(songs)}] {full_title}")
                    
                    final_lyrics = self.download_and_process_lyrics(
                        song['url'], song['song_name'], 
                        song['singer'], writer['name']
                    )
                    
                    if final_lyrics:
                        with open(lyrics_file, 'w', encoding='utf-8') as f:
                            f.write(final_lyrics)
                        print(f"      âœ“ å·²ä¿å­˜")
                        self.stats['songs_downloaded'] += 1
                    else:
                        print(f"      âœ— å¤±è´¥")
                
                self.stats['writers_processed'] += 1
        
        print(f"\n{'='*60}")
        print("âœ… å®Œæˆï¼")
        print(f"ä½œè¯äºº: {self.stats['writers_processed']}")
        print(f"æ­Œæ›²: {self.stats['songs_downloaded']}")
        print(f"{'='*60}")


if __name__ == '__main__':
    crawler = KugeciCrawlerFinal(
        output_dir='lyrics_final',
        start_page=1,
        end_page=1,  # å…ˆæµ‹è¯•
        delay=2.0
    )
    crawler.crawl()

